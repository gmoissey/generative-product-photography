{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Image\n",
    "from PIL import Image\n",
    "\n",
    "# For text extraction\n",
    "import pytesseract\n",
    "import easyocr\n",
    "\n",
    "# For EfficientNet\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input, decode_predictions\n",
    "import cv2\n",
    "\n",
    "# For Caption Generation\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms, models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Image and OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'watch.png'\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\djord\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'  # Replace with your Tesseract path, if necessary\n",
    "# Text extraction with Tesseract and EasyOCR\n",
    "text_tesseract = pytesseract.image_to_string(image)\n",
    "reader = easyocr.Reader(['en'])\n",
    "results = reader.readtext(image_path)\n",
    "\n",
    "# Extract text from the results and join it into a single string\n",
    "text_easyocr = ' '.join(result[1] for result in results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet pre-trained on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image classification with pretrained EfficientNet on ImageNet data\n",
    "model = EfficientNetB0(weights='imagenet')\n",
    "\n",
    "# EfficientNet preprocessing.\n",
    "img = cv2.imread(image_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "img = cv2.resize(img, (224, 224))  # Resize to the input size of EfficientNetB0\n",
    "img = preprocess_input(img)  # Preprocess according to EfficientNet requirements\n",
    "\n",
    "# Expand dimensions to create a batch (expects batch input)\n",
    "img = tf.expand_dims(img, axis=0)\n",
    "\n",
    "# Make predictions on the image\n",
    "predictions = model.predict(img)\n",
    "\n",
    "# Decode and display the top-5 predicted classes\n",
    "decoded_predictions = decode_predictions(predictions, top=5)[0]\n",
    "efficientNet = []\n",
    "for prediction in decoded_predictions:\n",
    "    class_label, probability = prediction[1], prediction[2]\n",
    "    efficientNet.append(f'{class_label}: {probability:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNetV2 custom trained on \"Retail Products Classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\djord/.cache\\torch\\hub\\pytorch_vision_main\n",
      "f:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Custom image classification\n",
    "# Define the class labels\n",
    "class_labels = [\n",
    "    'Arts, Crafts & Sewing', 'Cell Phones & Accessories', 'Clothing, Shoes & Jewelry',\n",
    "    'Tools & Home Improvement', 'Health & Personal Care', 'Baby Products', 'Baby',\n",
    "    'Patio, Lawn & Garden', 'Beauty', 'Sports & Outdoors', 'Electronics', 'All Electronics',\n",
    "    'Automotive', 'Toys & Games', 'All Beauty', 'Office Products', 'Appliances',\n",
    "    'Musical Instruments', 'Industrial & Scientific', 'Grocery & Gourmet Food', 'Pet Supplies'\n",
    "]\n",
    "\n",
    "# Define the MobileNetV2 model architecture\n",
    "model = torch.hub.load('pytorch/vision', 'mobilenet_v2', weights=False)  # Load MobileNetV2\n",
    "num_features = model.classifier[1].in_features  # Get the number of input features for the classifier\n",
    "model.classifier[1] = torch.nn.Linear(num_features, len(class_labels))  # Modify the classifier for your number of classes\n",
    "\n",
    "# Load pretrained weights\n",
    "model.load_state_dict(torch.load('categorization.pth', map_location=torch.device('cpu'))) \n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Update the transform for data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "imageMobileNet = transform(image)\n",
    "imageMobileNet = imageMobileNet.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    logits = model(imageMobileNet)\n",
    "\n",
    "# Convert logits to class probabilities\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "probs = probs.numpy()[0]\n",
    "# Map class probabilities to class labels\n",
    "top_classes = np.argsort(probs)[::-1][:3]  # Get the indices of the top 3 classes\n",
    "predicted_category = [class_labels[class_idx] for class_idx in top_classes]\n",
    "predicted_probabilities = [probs[class_idx] for class_idx in top_classes]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLIP using the BLIP-2, OPT-2.7b checkpoint \n",
    "Can also use smaller checkpoints. Use huggingface to specify a different checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fcfda2ef53475a8ef136300dcf58ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\transformers\\generation\\utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preprocess and classify the image using your custom model\n",
    "# Map the model's output to human-readable category names\n",
    "\n",
    "# Image captioning with BLIP/CLIP\n",
    "\n",
    "# Can use saved model or download checkpoint from Salesforce.\n",
    "#processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "#model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "local_model_dir = \"./models\"  # Local directory of model. \n",
    "\n",
    "# Load the model and processor from the local directory\n",
    "processor = Blip2Processor.from_pretrained(local_model_dir)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(local_model_dir)\n",
    "\n",
    "text = \"an image of\"\n",
    "model.to(\"cuda\") # Use GPU\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(**inputs.to(\"cuda\")) # Use GPU\n",
    "\n",
    "caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tesseract: ', 'EasyOCR: ', 'MobileNetV2(Categories): All Beauty (68.95%)', 'MobileNetV2(Categories): Toys & Games (12.68%)', 'MobileNetV2(Categories): Sports & Outdoors (9.19%)', 'EfficientNet(ImageNet): stopwatch: 55.24%)', 'EfficientNet(ImageNet): magnetic_compass: 9.21%)', 'EfficientNet(ImageNet): analog_clock: 6.13%)', 'BLIP: a black and white watch sitting on a wooden table']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Format the output\n",
    "results = [\n",
    "    f\"Tesseract: {text_tesseract}\",\n",
    "    f\"EasyOCR: {text_easyocr}\",\n",
    "    f\"MobileNetV2(Categories): {predicted_category[0]} ({predicted_probabilities[0]:.2%})\",\n",
    "    f\"MobileNetV2(Categories): {predicted_category[1]} ({predicted_probabilities[1]:.2%})\",\n",
    "    f\"MobileNetV2(Categories): {predicted_category[2]} ({predicted_probabilities[2]:.2%})\",\n",
    "    f\"EfficientNet(ImageNet): {efficientNet[0]})\",\n",
    "    f\"EfficientNet(ImageNet): {efficientNet[1]})\",\n",
    "    f\"EfficientNet(ImageNet): {efficientNet[2]})\",\n",
    "    f\"BLIP: {caption}\"\n",
    "]\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
