{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000\n"
     ]
    }
   ],
   "source": [
    "image_files = []\n",
    "for file in os.listdir(\"./retail-products-classification/train\"):\n",
    "    if file.endswith(\".jpg\"):\n",
    "        image_files.append(file.replace(\".jpg\", \"\"))\n",
    "print(len(image_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46229\n",
      "['Arts, Crafts & Sewing' 'Cell Phones & Accessories'\n",
      " 'Clothing, Shoes & Jewelry' 'Tools & Home Improvement'\n",
      " 'Health & Personal Care' 'Baby Products' 'Baby' 'Patio, Lawn & Garden'\n",
      " 'Beauty' 'Sports & Outdoors' 'Electronics' 'All Electronics' 'Automotive'\n",
      " 'Toys & Games' 'All Beauty' 'Office Products' 'Appliances'\n",
      " 'Musical Instruments' 'Industrial & Scientific' 'Grocery & Gourmet Food'\n",
      " 'Pet Supplies']\n"
     ]
    }
   ],
   "source": [
    "# Define constants and hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define early stopping parameters\n",
    "early_stopping_patience = 5  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')  # Initialize the best validation loss\n",
    "no_improvement_count = 0  # Initialize the count for epochs with no improvement\n",
    "\n",
    "# Data Preparation\n",
    "\n",
    "# Organize your dataset into folders as described earlier\n",
    "# Create a CSV file train.csv with columns ImgId and categories\n",
    "\n",
    "# Define the path to your dataset folder\n",
    "dataset_path = \"retail-products-classification\"\n",
    "\n",
    "# Read the CSV file and extract labels/classes\n",
    "csv_file = os.path.join(dataset_path, \"train.csv\")\n",
    "df = pd.read_csv(csv_file)\n",
    "classes = df[\"categories\"].unique()  # Extract unique classes/categories\n",
    "print(len(df))\n",
    "\n",
    "# Create a dictionary to map class names to class indices\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "num_classes = len(classes)\n",
    "print(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the transform for data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, class_to_idx, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "\n",
    "        #print(self.data.head())\n",
    "\n",
    "        self.data = self.data[self.data['ImgId'].isin(image_files)]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data.iloc[idx, 0])\n",
    "        image = Image.open(\"{}.jpg\".format(img_name)).convert('RGB')\n",
    "        label = self.class_to_idx[self.data.iloc[idx, 3]]  # Map class name to class index\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_ratio = 0.6  # Percentage of data for training\n",
    "val_ratio = 0.2   # Percentage of data for validation\n",
    "test_ratio = 0.2  # Percentage of data for testing\n",
    "\n",
    "train_data, temp_data = train_test_split(df, test_size=1 - train_ratio, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=test_ratio / (test_ratio + val_ratio), random_state=42)\n",
    "\n",
    "# Create DataLoaders for training, validation, and test sets\n",
    "train_dataloader = DataLoader(\n",
    "    CustomDataset(csv_file=csv_file, root_dir=os.path.join(dataset_path, \"train\"), class_to_idx=class_to_idx, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    CustomDataset(csv_file=csv_file, root_dir=os.path.join(dataset_path, \"train\"), class_to_idx=class_to_idx, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    CustomDataset(csv_file=csv_file, root_dir=os.path.join(dataset_path, \"train\"), class_to_idx=class_to_idx, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "\n",
    "# Load pre-trained MobileNetV2 model\n",
    "mobilenet_v2 = models.mobilenet_v2(weights=\"MobileNet_V2_Weights.DEFAULT\")\n",
    "\n",
    "# Modify the output layer for your specific number of classes\n",
    "mobilenet_v2.classifier[1] = nn.Linear(mobilenet_v2.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "mobilenet_v2 = mobilenet_v2.to(device)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobilenet_v2.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 2.027583599090576, Validation Loss: 1.9107921517539914\n",
      "Epoch [2/30], Loss: 2.0178229808807373, Validation Loss: 1.7223138462380425\n",
      "Epoch [3/30], Loss: 1.4634932279586792, Validation Loss: 1.5301806280291543\n",
      "Epoch [4/30], Loss: 1.4913073778152466, Validation Loss: 1.4096607604920184\n",
      "Epoch [5/30], Loss: 1.0657446384429932, Validation Loss: 1.3031420273265126\n",
      "Epoch [6/30], Loss: 1.3714121580123901, Validation Loss: 1.1540862173636453\n",
      "Epoch [7/30], Loss: 1.212152123451233, Validation Loss: 1.0180759560671455\n",
      "Epoch [8/30], Loss: 0.8762522339820862, Validation Loss: 0.9363038296338173\n",
      "Epoch [9/30], Loss: 0.6563171744346619, Validation Loss: 0.7866326612274972\n",
      "Epoch [10/30], Loss: 0.7052903771400452, Validation Loss: 0.7141301410623117\n",
      "Epoch [11/30], Loss: 0.592488169670105, Validation Loss: 0.638092515333318\n",
      "Epoch [12/30], Loss: 0.700343668460846, Validation Loss: 0.5341538782348546\n",
      "Epoch [13/30], Loss: 0.48541373014450073, Validation Loss: 0.49418130629360446\n",
      "Epoch [14/30], Loss: 0.29679685831069946, Validation Loss: 0.42481179575110245\n",
      "Epoch [15/30], Loss: 0.3669390380382538, Validation Loss: 0.40528336695628775\n",
      "Epoch [16/30], Loss: 0.12662750482559204, Validation Loss: 0.34946193144304194\n",
      "Epoch [17/30], Loss: 0.13657523691654205, Validation Loss: 0.32253419506872144\n",
      "Epoch [18/30], Loss: 0.027543770149350166, Validation Loss: 0.27870838226777445\n",
      "Epoch [19/30], Loss: 0.26403650641441345, Validation Loss: 0.2831097808123906\n",
      "Epoch [20/30], Loss: 0.1162862777709961, Validation Loss: 0.2695843509575019\n",
      "Epoch [21/30], Loss: 0.18679913878440857, Validation Loss: 0.2290672943115189\n",
      "Epoch [22/30], Loss: 0.14239615201950073, Validation Loss: 0.22527916092207118\n",
      "Epoch [23/30], Loss: 0.239127516746521, Validation Loss: 0.22885833002246705\n",
      "Epoch [24/30], Loss: 0.10580193996429443, Validation Loss: 0.19104009396868268\n",
      "Epoch [25/30], Loss: 0.05094235762953758, Validation Loss: 0.18113778566733083\n",
      "Epoch [26/30], Loss: 0.17059630155563354, Validation Loss: 0.1558597681182809\n",
      "Epoch [27/30], Loss: 0.04463095963001251, Validation Loss: 0.15897540061933932\n",
      "Epoch [28/30], Loss: 0.41562604904174805, Validation Loss: 0.15924321219994653\n",
      "Epoch [29/30], Loss: 0.011257912963628769, Validation Loss: 0.1496811353800831\n",
      "Epoch [30/30], Loss: 0.1339758038520813, Validation Loss: 0.13773802800780702\n"
     ]
    }
   ],
   "source": [
    "# Training Loop with Early Stopping\n",
    "for epoch in range(num_epochs):\n",
    "    mobilenet_v2.train()\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = mobilenet_v2(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    mobilenet_v2.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = mobilenet_v2(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "\n",
    "    # Check if validation loss has improved\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement_count = 0\n",
    "        # Save the model checkpoint if validation loss improved\n",
    "        torch.save(mobilenet_v2.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Validation Loss: {val_loss}')\n",
    "\n",
    "    # Check for early stopping\n",
    "    if no_improvement_count >= early_stopping_patience:\n",
    "        print(f'Early stopping triggered after {early_stopping_patience} epochs without improvement.')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 95.70714285714286%\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "    Arts, Crafts & Sewing       0.96      0.95      0.96      2000\n",
      "Cell Phones & Accessories       0.94      0.97      0.96      2000\n",
      "Clothing, Shoes & Jewelry       0.97      0.97      0.97      2000\n",
      " Tools & Home Improvement       0.88      0.98      0.93      2000\n",
      "   Health & Personal Care       0.95      0.97      0.96      2000\n",
      "            Baby Products       0.97      0.96      0.96      2000\n",
      "                     Baby       0.98      0.96      0.97      2000\n",
      "     Patio, Lawn & Garden       0.97      0.94      0.95      2000\n",
      "                   Beauty       0.96      0.93      0.95      2000\n",
      "        Sports & Outdoors       0.95      0.95      0.95      2000\n",
      "              Electronics       0.94      0.96      0.95      2000\n",
      "          All Electronics       0.97      0.91      0.94      2000\n",
      "               Automotive       0.97      0.95      0.96      2000\n",
      "             Toys & Games       0.97      0.96      0.96      2000\n",
      "               All Beauty       0.92      0.97      0.95      2000\n",
      "          Office Products       0.98      0.96      0.97      2000\n",
      "               Appliances       0.98      0.93      0.95      2000\n",
      "      Musical Instruments       0.98      0.96      0.97      2000\n",
      "  Industrial & Scientific       0.92      0.97      0.95      2000\n",
      "   Grocery & Gourmet Food       0.97      0.97      0.97      2000\n",
      "             Pet Supplies       0.98      0.96      0.97      2000\n",
      "\n",
      "                 accuracy                           0.96     42000\n",
      "                macro avg       0.96      0.96      0.96     42000\n",
      "             weighted avg       0.96      0.96      0.96     42000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1904    0    4   14    4    2    5    0    1    1    3    2    2    5\n",
      "    10   11    1    1   15    7    8]\n",
      " [   2 1943    2    8    4    1    1    1    0    2   29    2    1    0\n",
      "     0    1    0    0    2    1    0]\n",
      " [   3    2 1943    9    1    1    2    2    0    7    4    1    4    4\n",
      "     8    0    1    1    4    1    2]\n",
      " [   3    2    1 1968    5    0    0    2    0    2    2    2    2    0\n",
      "     0    0    2    0    9    0    0]\n",
      " [   2    0    3    2 1940    5    3    0   19    3    1    0    1    1\n",
      "     6    1    0    0    9    2    2]\n",
      " [   5    2    3    8    4 1912   14    6    9    2    2    1    1    6\n",
      "     9    1    5    1    5    2    2]\n",
      " [   1    1    9    8    4   11 1929    3    0    2    3    0    2    3\n",
      "     2    1    7    0   12    0    2]\n",
      " [   4    4    3   31    4    2    3 1879    5    9    3    2    0    0\n",
      "     0    2    3    7   32    5    2]\n",
      " [   4    0    0    4   22    5    0    0 1860    3    1    1    0    0\n",
      "    88    2    0    0    3    7    0]\n",
      " [   6    8    9   14   13    3    2    5    1 1895    3    4   10    4\n",
      "     1    0    2    4   10    3    3]\n",
      " [   0   44    0   11    1    1    0    1    0    2 1927    4    2    0\n",
      "     1    2    0    1    2    0    1]\n",
      " [   5   32    6   12    9    4    0    3    0    6   36 1819    8    3\n",
      "    10   13   11    8    9    4    2]\n",
      " [   4    4    0   17    1    1    1    7    1   19    6    4 1904    0\n",
      "     2    2    2    2   19    0    4]\n",
      " [   4    1    4   15    2    9   13    5    1    3    6    3    2 1917\n",
      "     3    0    1    4    1    3    3]\n",
      " [   4    1    0    5    2    2    0    2   19    3    0    1    2    0\n",
      "  1947    0    0    3    5    2    2]\n",
      " [   7    5    1   12    3    1    2    1    2    3    7    4    0   26\n",
      "     2 1912    1    2    3    3    3]\n",
      " [   9    1    3   53    3    2    1   10    2   10    1    8    3    0\n",
      "     2    3 1855    4   21    2    7]\n",
      " [   3    4    6   15    6    0    0    2    0   12    6    7    1    0\n",
      "     4    1    2 1928    3    0    0]\n",
      " [  10    2    0   10    3    0    0    1    0    6    1    1    7    0\n",
      "     1    1    1    3 1950    1    2]\n",
      " [   2    0    0    3   12    3    1    0    3    1    0    1    3    4\n",
      "    11    0    0    0    4 1948    4]\n",
      " [   2    1    1    6    6    4    1   13    6    8    2    3    5    2\n",
      "     3    3    3    2    5    7 1917]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "mobilenet_v2.load_state_dict(torch.load('best_model.pth'))  # Load the best model checkpoint\n",
    "mobilenet_v2.eval()\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = mobilenet_v2(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "report = classification_report(all_labels, all_predictions, target_names=classes)\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Test Accuracy: {accuracy*100}%')\n",
    "print(report)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
